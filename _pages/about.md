---
permalink: /
title: "Home"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# Bio

My name is Ge Chunjiang. I am currently a fifth year PhD candidate in Tsinghua University. My current interest is working towards enabling machine learning models to understand the open world, interact with the open world using tools. Achieving this goal requires integrating technologies across large language models, Embodied AI, Agents, multi-modality.

我是葛春江，目前是清华大学五年级的博士生。当前的我当前的兴趣是致力于使机器学习模型可以理解开放世界，并通过利用工具和开放世界交互。实现这个目标需要将大语言模型，具身智能，智能体，多模态多个领域的技术融合。

I am now a Ph.D candidate of Department of Automation, Tsinghua University, advised by Prof. [Gao huang](http://www.gaohuang.net/). Before coming to Department of Automation, I received B.S. in Department of Physics, Tsinghua University.

我目前就读于清华大学自动化系。我的导师是黄高教授。我在清华大学物理系获得数理基础科学学位。

If you're interested in my work or personal development, feel free to contact me. I can arrange 30 minutes per week to communicate with you. You can contact me by email.

我每周可以安排30分钟的时间和同学们交流，可以给我发邮件联系。

I am seeking postdoc opportunities.

我在寻求博士后的机会。

# News

- [2024/05] I am excited to announce that our project and paper, [ConvLLaVA](https://arxiv.org/abs/2405.15738), has been released. We employ a hierarchical backbone for High resolution understanding, which is efficient and effective. Welcome cooperation!
- [2023/11] I am working on an LLM project to generate data, spanning across open-source LLM, multi-modality, quantization, and deployment. I wish to release it in December.
<!-- - [2023/08] I become a contributor of project [OpenRLHF](https://github.com/OpenLLMAI/OpenRLHF/tree/main).  -->
- [2023/06] I establish a github repo for collecting papers on [foundation models](https://github.com/John-Ge/awesome-foundation-models). Welcome pull requests and collaboration. 我建立了一个github仓库，收集了关于基础模型的一些论文，欢迎大家贡献与合作。

# Publications

## Highlight

**ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models**\
Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, Bo Zheng \
**TL; DR:** We propose to employ a five stage ConvNeXt as the visual encoder of LMM to compress visual tokens, greatly improves performance on high resolution benchmarks and efficiency.\
<a href="http://arxiv.org/abs/2405.15738"> 
    <img src="https://img.shields.io/badge/arXiv-2405.15738-b31b1b.svg?logo=arXiv">
</a>
<a href="https://github.com/alibaba/conv-llava"> 
    <img src="https://img.shields.io/badge/Github-ConvLLaVA-181717.svg?logo=GitHub">
</a>
<a href="https://huggingface.co/collections/ConvLLaVA/convllava-66519ef0ccdee62544bd19bf"> 
    <img src="https://img.shields.io/badge/🤗%20Hugging%20Face-Models-ffd21e">
</a>
<a href="https://modelscope.cn/organization/ConvLLaVA?tab=model"> 
    <img src="https://img.shields.io/badge/🤖%20ModelScope-Models-5f4cf2.svg">
</a>
<a href="https://github.com/alibaba/conv-llava/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/alibaba/conv-llava?color=ccf" />
</a>

**Domain Adaptation via Prompt Learning**\
Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, Gao Huang.  \
**TL; DR:** We propose a novel domain adaptation method, DAPrompt, which learns a set of domain-specific prompts to avoid information loss resulted from domain alignment.\
<a href="https://arxiv.org/abs/2202.06687"> 
    <img src="https://img.shields.io/badge/arXiv-2202.06687-b31b1b.svg?logo=arXiv">
</a>
  <a href="https://github.com/LeapLabTHU/DAPrompt"> 
    <img src="https://img.shields.io/badge/Github-DAPrompt-181717.svg?logo=GitHub">
</a>
<a href="https://github.com/LeapLabTHU/DAPrompt/stargazers">
    <img alt="GitHub stars" src="https://img.shields.io/github/stars/LeapLabTHU/DAPrompt?color=ccf" />
</a>

## Preprint

**ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models**\
Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, Bo Zheng \
[arXiv](https://arxiv.org/abs/2405.15738) [code](https://github.com/alibaba/conv-llava)

**Cross-Modal Adapter for Text-Video Retrieval**\
H Jiang, J Zhang, R Huang, C Ge, Z Ni, J Lu, J Zhou, S Song, G Huang \
[arXiv](https://arXiv.org/abs/2211.09623) [code](https://github.com/LeapLabTHU/Cross-Modal-Adapter)

## Conference Papers

**On the Integration of Self-Attention and Convolution**\
Pan, Xuran, Chunjiang Ge, Rui Lu, Shiji Song, Guanfu Chen, Zeyi Huang, and Gao Huang.  \
IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2022)\
[arXiv](https://arxiv.org/abs/2111.14556) [Code](https://github.com/leaplabthu/acmix)

**Causal Intervention for Human Trajectory Prediction with Cross Attention Mechanism**\
Chunjiang Ge, Shiji Song and Gao Huang. \
AAAI Conference on Artificial Intelligence (AAAI 2023)\
[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25142)

## Journal Papers

**Domain Adaptation via Prompt Learning**\
Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, Gao Huang.  \
IEEE Transactions on Neural Networks and Learning Systems (TNNLS) \
[arXiv](https://arxiv.org/abs/2202.06687) [code](https://github.com/LeapLabTHU/DAPrompt)

**Large scale air pollution prediction with deep convolutional networks**\
Gao Huang$^\ast$, Chunjiang Ge$^\ast$, Tianyu Xiong, Shiji Song, Le Yang, Baoxian Liu, Wenjun Yin and Cheng Wu.  \
Science China Information Sciences. (IF：8.8) \
[Paper](https://link.springer.com/article/10.1007/s11432-020-2951-1)

---

<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5fymzl2m77g&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>